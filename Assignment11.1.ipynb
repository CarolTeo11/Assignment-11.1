{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================================================================================\n",
    "========================================================================================================================\n",
    "Task 1. Build different predictive model using Linear Regression by varying the degree of PolynomialFeatures, applying PCA to reduce dimsensionality before applying Linear Regression, LASSO model to determine top factors and Ridge Model.\n",
    "\n",
    "Measure of effectiveness:\n",
    "\n",
    "- Mean Squared Error of the Training datasets\n",
    "\n",
    "- Mean Squared Error of the Training datasets\n",
    "\n",
    "- R^2 value (for linear regression) to determine how much of the variance can be explained by the model\n",
    "\n",
    "Task 2. Using the above metrics, determine the best model\n",
    "\n",
    "Task 3. For the best model, present the coefficients\n",
    "\n",
    "========================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.filters.filtertools import convolution_filter\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "from statsmodels.tsa.seasonal import _extrapolate_trend\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pd.read_csv('data/vehicles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(cars, x='year', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to extreme outliers, it is recommended that we limit the car prices to no more than $250,000\n",
    "px.scatter(cars[cars['price'] <= 250_000], x='year', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(cars[cars['price'] <= 250_000], x='year', y='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the code below to determine the spread of car prices.  I have also plotted it separately in box and histogram above. \n",
    "#px.scatter(cars[cars['price'] <= 250_000], x='year', y='price', marginal_y='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. digitise all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(cars[['manufacturer','condition', 'cylinders', 'fuel', 'title_status', 'transmission',\n",
    "                             'drive', 'size', 'type']])\n",
    "cars_dummies = pd.concat([cars, dummies], axis=1)\n",
    "cars_dummies=cars_dummies.drop(columns = ['manufacturer','condition', 'cylinders', 'fuel', 'title_status', 'transmission',\n",
    "                             'drive', 'size', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove unnecessary data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars = cars_dummies.drop(columns = ['id', 'region','model','VIN', 'paint_color','state'])\n",
    "df_cars = df_cars.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain reasonable values of price\n",
    "\n",
    "# I remove all car prices > $250,000 as any higher doesn't sound realistic and i assumed it was data entry error\n",
    "# I limited the data to car prices > $0 for 2 reasons - 1. giving a car for free is possible but it wont tell us much about\n",
    "# price predictions; 2. I decided to apply a logarithmic model on the car price so that after we apply an exponential function \n",
    "# to reverse the logarithm, the price is positive.  Log cannot be applied on value zero. (I have tried an earlier model and \n",
    "# astoundingly, many of the predicted prices were negative.)\n",
    "\n",
    "df_cars=df_cars[(df_cars['price']<=250_000) & (df_cars['price']>0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.log(df_cars['price'])\n",
    "X=df_cars.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(df_cars.corr())['price'].sort_values(ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Repeat steps 1 and 2 but this time, remove manufacturer data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies2=pd.get_dummies(cars[['condition', 'cylinders', 'fuel', 'title_status', 'transmission',\n",
    "                             'drive', 'size', 'type']])\n",
    "cars_dummies2 = pd.concat([cars, dummies2], axis=1)\n",
    "cars_dummies2=cars_dummies2.drop(columns = ['condition', 'cylinders', 'fuel', 'title_status', 'transmission',\n",
    "                             'drive', 'size', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars2 = cars_dummies2.drop(columns = ['id', 'region','model','VIN', 'paint_color','state', 'manufacturer'])\n",
    "df_cars2 = df_cars2.dropna()\n",
    "df_cars2=df_cars2[(df_cars2['price']<=250_000) & (df_cars2['price']>0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=np.log(df_cars2['price'])\n",
    "X2=df_cars2.drop(columns=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = []\n",
    "test_mse = []\n",
    "explained_variance = []\n",
    "model =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Linear Regression with 92 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (Degree = 1) using 92 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR1_model = Pipeline([\n",
    "    ('transform', PolynomialFeatures(degree=1, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "LR1_model.fit(X_train, y_train)\n",
    "\n",
    "LR1_train_mse= round(mean_squared_error(LR1_model.predict(X_train), y_train), 4)\n",
    "LR1_test_mse=round(mean_squared_error(LR1_model.predict(X_test), y_test),4)\n",
    "LR1_EV = explained_variance_score(y_train, LR1_model.predict(X_train))\n",
    "\n",
    "train_mse.append(LR1_train_mse)\n",
    "test_mse.append(LR1_test_mse)\n",
    "explained_variance.append(LR1_EV)\n",
    "model.append(\"Model 1 - LR1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(X_train['year'], np.exp(y_train), c =\"blue\", alpha=0.3, label='observed values')\n",
    "plt.scatter(X_train['year'], np.exp(LR1_model.predict(X_train)), c =\"orange\", alpha=0.3, label = 'predicted value')\n",
    "plt.title('Predicted vs Observed (Using ordinary linear regression on 92 features); Training MSE = ' + str(LR1_train_mse) +\n",
    "          '; Development MSE = ' + str(LR1_test_mse))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Car Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Linear Regression with 50 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (Degree = 1) using 50 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR2_model = Pipeline([\n",
    "    ('transform', PolynomialFeatures(degree=1, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "LR2_model.fit(X_train2, y_train2)\n",
    "\n",
    "LR2_train_mse= round(mean_squared_error(LR2_model.predict(X_train2), y_train2), 4)\n",
    "LR2_test_mse=round(mean_squared_error(LR2_model.predict(X_test2), y_test2),4)\n",
    "LR2_EV = explained_variance_score(y_train2, LR2_model.predict(X_train2))\n",
    "\n",
    "train_mse.append(LR2_train_mse)\n",
    "test_mse.append(LR2_test_mse)\n",
    "explained_variance.append(LR2_EV)\n",
    "model.append(\"Model 2 - LR2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(X_train2['year'], np.exp(y_train2), c =\"blue\", alpha=0.3, label='observed values')\n",
    "plt.scatter(X_train2['year'], np.exp(LR2_model.predict(X_train2)), c =\"orange\", alpha=0.3, label = 'predicted value')\n",
    "plt.title('Predicted vs Observed (Using ordinary linear regression on 50 features); Training MSE = ' + str(LR2_train_mse) +\n",
    "          '; Development MSE = ' + str(LR2_test_mse))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Car Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Linear Regression with 50 features and applying PolynomialFeatures(degree = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR3_model = Pipeline([\n",
    "    ('transform', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('linreg', LinearRegression())\n",
    "])\n",
    "LR3_model.fit(X_train2, y_train2)\n",
    "\n",
    "LR3_train_mse= round(mean_squared_error(LR3_model.predict(X_train2), y_train2), 4)\n",
    "LR3_test_mse=round(mean_squared_error(LR3_model.predict(X_test2), y_test2),4)\n",
    "LR3_EV= explained_variance_score(y_train2, LR3_model.predict(X_train2))\n",
    "\n",
    "train_mse.append(LR3_train_mse)\n",
    "test_mse.append(LR3_test_mse)\n",
    "explained_variance.append(LR3_EV)\n",
    "model.append(\"Model 3 - LR3Deg2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(X_train2['year'], np.exp(y_train2), c =\"blue\", alpha=0.3, label='observed values')\n",
    "plt.scatter(X_train2['year'], np.exp(LR3_model.predict(X_train2)), c =\"orange\", alpha=0.3, label = 'predicted value')\n",
    "plt.title('Predicted vs Observed (Using linear regression with degree =2 on 50 features); Training MSE = ' + str(LR3_train_mse) +\n",
    "          '; Development MSE = ' + str(LR3_test_mse))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Car Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: LASSO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lasso = Pipeline([\n",
    "    ('transform', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"scaler\", StandardScaler()), \n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "pipe_lasso.fit(X_train2, y_train2)\n",
    "\n",
    "lasso_train_mse= round(mean_squared_error(pipe_lasso.predict(X_train2), y_train2), 4)\n",
    "lasso_test_mse=round(mean_squared_error(pipe_lasso.predict(X_test2), y_test2),4)\n",
    "lasso_EV= explained_variance_score(y_train2, pipe_lasso.predict(X_train2))\n",
    "\n",
    "train_mse.append(lasso_train_mse)\n",
    "test_mse.append(lasso_test_mse)\n",
    "explained_variance.append(lasso_EV)\n",
    "model.append(\"Model 4 - Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef = pipe_lasso.named_steps['lasso'].coef_\n",
    "lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(X_train2['year'], np.exp(y_train2), c =\"blue\", alpha=0.3, label='observed values')\n",
    "plt.scatter(X_train2['year'], np.exp(pipe_lasso.predict(X_train2)), c =\"orange\", alpha=0.3, label = 'predicted value')\n",
    "plt.title('Predicted vs Observed (Using Lasso model); Training MSE = ' + str(lasso_train_mse) +\n",
    "          '; Development MSE = ' + str(lasso_test_mse))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Car Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Ridge model with varying alphas\n",
    "#### Please note that the alogrithm below will take a few minutes to run as it is being trained on a high degree model and running an optimisation to find the best solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_model = Pipeline([\n",
    "    ('transform', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "alpha_value = {'ridge__alpha': [0.1,1,10]}\n",
    "\n",
    "model_finder = GridSearchCV(estimator = ridge_model, \n",
    "                           param_grid=alpha_value,\n",
    "                           scoring = \"neg_mean_squared_error\"\n",
    "                           )\n",
    "\n",
    "model_finder.fit(X_train2, y_train2)\n",
    "\n",
    "best_model=model_finder.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_alpha = model_finder.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_train_mse = round(mean_squared_error(best_model.predict(X_train2), y_train2),4)\n",
    "ridge_test_mse = round(mean_squared_error(best_model.predict(X_test2), y_test2),4)\n",
    "ridge_EV= explained_variance_score(y_train2, best_model.predict(X_train2))\n",
    "\n",
    "train_mse.append(ridge_train_mse)\n",
    "test_mse.append(ridge_test_mse)\n",
    "explained_variance.append(ridge_EV)\n",
    "model.append(\"Model 5 - Ridge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.scatter(X_train2['year'], np.exp(y_train2), c =\"blue\", alpha=0.3, label='observed values')\n",
    "plt.scatter(X_train2['year'], np.exp(best_model.predict(X_train2)), c =\"orange\", alpha=0.3, label = 'predicted value')\n",
    "plt.title('Predicted vs Observed (Using ridge model with alpha = ' + str(best_alpha) + '); Training MSE = ' + str(ridge_train_mse) +\n",
    "          '; Development MSE = ' + str(ridge_test_mse))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Car Price')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight on drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricsTable = pd.DataFrame({\n",
    "    'model': model,\n",
    "    'train_mse': train_mse,\n",
    "    'test_mse':test_mse,\n",
    "    'explained_variance': explained_variance\n",
    "})\n",
    "\n",
    "MetricsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the best performing model - the ridge model (PolynomialFeatures(degree=2), alpha =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_value = pd.DataFrame({\n",
    "    'features': best_model.named_steps['transform'].get_feature_names_out(),\n",
    "    'coefficient': best_model.named_steps['ridge'].coef_\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_value['abs_coef'] = np.abs(ridge_value['coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_value.sort_values(by = 'abs_coef', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### refer to https://github.com/CarolTeo11/Assignment-11.1 for the model and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
